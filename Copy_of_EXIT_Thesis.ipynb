{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EXIT Thesis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelegaruti/Thesis-2021-Garuti/blob/main/Copy_of_EXIT_Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK_LJMRy7X6n"
      },
      "source": [
        "#import modules here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwq0Q-skKyqv"
      },
      "source": [
        "#Time Aware layer for T_LSTM models:\n",
        "'''\n",
        "TLSTM_layer is obtained from the following Github Repository:\n",
        "\n",
        "Nguyen, A., Chatterjee, S., Weinzierl, S., Schwinn, L., Matzner, M., & Eskofier, B.(2020).\n",
        "Time matters: Time-aware lstms for predictive business processmonitoring; TLSTM_layer [Source Code]. Retrievable at:\n",
        "https://github.com/annguy/time-aware-pbpm/blob/master/src/Models/TLSTM_layer.py. [Accessed on May 21st, 2021]\n",
        "\n",
        "The MIT License (MIT)\n",
        "Copyright (c) 2020, An Nguyen, Srijeet Chatterjee\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of \n",
        "this software and associated documentation files (the \"Software\"),\n",
        "to deal in the Software without restriction, including without limitation the rights to use,\n",
        "copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n",
        "and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies\n",
        "or substantial portions of the Software.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n",
        "TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
        "IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n",
        "WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE\n",
        "OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\n",
        "Link for detailed installation instructions of conda environment: https://github.com/annguy/time-aware-pbpm\n",
        "'''\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TLSTM_layer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Time-ware LSTM inherits from tf.keras.layers.Layer. Implmentation is based on [1]\n",
        "    [1] Baytas, Inci M., Cao Xiao, Xi Zhang, Fei Wang, Anil K. Jain, and Jiayu Zhou. “Patient Subtyping\n",
        "    via Time-Aware LSTM Networks.”  KDD ’17\n",
        "    Attributes\n",
        "    ----------\n",
        "    input_dim : int\n",
        "        the dimension of the input layer.\n",
        "    hidden_dim : int\n",
        "        the dimension of the hidden layer.\n",
        "    trainable : bool\n",
        "        set the variable to be trainable or not.\n",
        "    Wi, Wf, Wog : tensor\n",
        "        weight matrices corresponding to the input, forget and output gates respectively,\n",
        "        modifying the input connections.\n",
        "    Ui, Uf, Uog : tensor\n",
        "        weight matrices corresponding to the input, forget and output gates respectively,\n",
        "        modifying the hidden connections.\n",
        "    bi, bf, bog : tensor\n",
        "        the bias vectors corresponding to the input, forget and output gates, respectively.\n",
        "    Wc, Uc, bc : tensor\n",
        "        the weight matrices and bias vector to compute the current cell memory.\n",
        "    W_decomp, b_decomp : tensor\n",
        "        subspace decompostion weight matrix and bias vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_dim,\n",
        "                 return_sequence=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 trainable=True,\n",
        "                 supports_masking=True,\n",
        "                 name=None):\n",
        "        \"\"\"Constructor of T-LSTM objects.\n",
        "        Parameters\n",
        "        ----------\n",
        "        hidden_dim : int\n",
        "            dimension of the hidden layer.\n",
        "        return_sequence : bool\n",
        "            weather to return all hidden states. Default: True\n",
        "        kernel_initializer : str\n",
        "            Initializer for the kernel weights matrix, used for the linear transformation of the inputs.\n",
        "             Default: glorot_uniform.\n",
        "         bias_initializer: str\n",
        "             Initializer for the bias vector. Default: zeros.\n",
        "        trainable :bool\n",
        "            set the variables (weight and bias) to be trainable (for training) or not (for testing).\n",
        "        supports_masking : bool\n",
        "            weather layer should supporting masking\n",
        "        name : str\n",
        "            layers name\n",
        "        \"\"\"\n",
        "        super(TLSTM_layer, self).__init__(name=name)\n",
        "        # Signal that the layer is safe for mask propagation\n",
        "        self.supports_masking = supports_masking\n",
        "        # set initializer\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        # set dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.return_sequence = return_sequence\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        self.input_dim = input_shape[-1]\n",
        "        # Set trainable weights\n",
        "        self.Wi = self.add_weight(shape=(self.input_dim, self.hidden_dim),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Input_Hidden_weight')\n",
        "        self.Ui = self.add_weight(shape=(self.hidden_dim, self.hidden_dim),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Input_State_weight')\n",
        "        self.bi = self.add_weight(shape=self.hidden_dim,\n",
        "                                  initializer=self.bias_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Input_Hidden_bias')\n",
        "\n",
        "        self.Wf = self.add_weight(shape=(self.input_dim, self.hidden_dim),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Forget_Hidden_weight')\n",
        "        self.Uf = self.add_weight(shape=(self.hidden_dim, self.hidden_dim),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Forget_State_weight')\n",
        "        self.bf = self.add_weight(shape=self.hidden_dim,\n",
        "                                  initializer=self.bias_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Forget_Hidden_bias')\n",
        "\n",
        "        self.Wog = self.add_weight(shape=(self.input_dim, self.hidden_dim),\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   trainable=True,\n",
        "                                   name='Output_Hidden_weight')\n",
        "        self.Uog = self.add_weight(shape=(self.hidden_dim, self.hidden_dim),\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   trainable=True,\n",
        "                                   name='Output_State_weight')\n",
        "        self.bog = self.add_weight(shape=self.hidden_dim,\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   trainable=True,\n",
        "                                   name='Output_Hidden_bias')\n",
        "\n",
        "        self.Wc = self.add_weight(shape=(self.input_dim, self.hidden_dim),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Cell_Hidden_weight')\n",
        "        self.Uc = self.add_weight(shape=(self.hidden_dim, self.hidden_dim),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Cell_State_weight')\n",
        "        self.bc = self.add_weight(shape=self.hidden_dim,\n",
        "                                  initializer=self.bias_initializer,\n",
        "                                  trainable=True,\n",
        "                                  name='Cell_Hidden_bias')\n",
        "\n",
        "        self.W_decomp = self.add_weight(shape=(self.hidden_dim, self.hidden_dim),\n",
        "                                        initializer=self.kernel_initializer,\n",
        "                                        trainable=True,\n",
        "                                        name='Decomposition_Hidden_weight')\n",
        "        self.b_decomp = self.add_weight(shape=self.hidden_dim,\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        trainable=True,\n",
        "                                        name='Decomposition_Hidden_bias_enc')\n",
        "\n",
        "    def tlstm_unit(self, prev_hidden_memory, concat_input):\n",
        "        \"\"\"The update equations for the time-aware LSTM.\n",
        "        Parameters\n",
        "        ----------\n",
        "        prev_hidden_memory : list of tensor\n",
        "            a list containing the previous hidden states and previous cell memory (1, 2, batch_size, hidden_dim)\n",
        "        concat_input : tensor\n",
        "            a tensor containing all the input data and the elapsed times (batch_size, input_dim+1)\n",
        "        Returns\n",
        "        -------\n",
        "        tensor\n",
        "            Updated prev_hidden_memory and the cell state concatenated (1, 2, batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        prev_hidden_state, prev_cell = tf.unstack(prev_hidden_memory)\n",
        "\n",
        "        batch_size = tf.shape(concat_input)[0]\n",
        "        x = tf.slice(concat_input, [0, 1], [batch_size, self.input_dim])  # (batch_size, features)\n",
        "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])  # (batch_size, 1)\n",
        "\n",
        "        # Dealing with time irregularity\n",
        "\n",
        "        # Map elapse time in days or months\n",
        "        T = self.map_elapse_time(t)\n",
        "\n",
        "        # Decompose the previous cell if there is an elapse time\n",
        "        C_ST = tf.nn.tanh(tf.matmul(prev_cell, self.W_decomp) + self.b_decomp)\n",
        "        C_ST_dis = tf.multiply(T, C_ST)\n",
        "        # if T is 0, then the weight is one\n",
        "        prev_cell = prev_cell - C_ST + C_ST_dis\n",
        "\n",
        "        # Input gate\n",
        "        i = tf.sigmoid(tf.matmul(x, self.Wi) + tf.matmul(prev_hidden_state, self.Ui) + self.bi)\n",
        "\n",
        "        # Forget Gate\n",
        "        f = tf.sigmoid(tf.matmul(x, self.Wf) + tf.matmul(prev_hidden_state, self.Uf) + self.bf)\n",
        "\n",
        "        # Output Gate\n",
        "        o = tf.sigmoid(tf.matmul(x, self.Wog) + tf.matmul(prev_hidden_state, self.Uog) + self.bog)\n",
        "\n",
        "        # Candidate Memory Cell\n",
        "        C = tf.nn.tanh(tf.matmul(x, self.Wc) + tf.matmul(prev_hidden_state, self.Uc) + self.bc)\n",
        "\n",
        "        # Current Memory cell\n",
        "        Ct = f * prev_cell + i * C\n",
        "\n",
        "        # Current Hidden state\n",
        "        current_hidden_state = o * tf.nn.tanh(Ct)\n",
        "\n",
        "        return tf.stack([current_hidden_state, Ct])\n",
        "\n",
        "    def call(self, x, time):\n",
        "        \"\"\" Compute the hidden states for the input sequence.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array\n",
        "            the input of shape (batch_size, sequence_length, input_dimension).\n",
        "        time : array\n",
        "            the elapsed time of shape (batch_size, sequence_length).\n",
        "        Returns\n",
        "        -------\n",
        "        tensor\n",
        "            The computed hidden states of shape (batch_size, seq_len, hidden_dim)\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        scan_input_ = tf.transpose(x, perm=[2, 0, 1])\n",
        "        scan_input = tf.transpose(scan_input_)  # scan_input: (seq_length, batch_size, input_dim)\n",
        "        scan_time = tf.transpose(time)\n",
        "\n",
        "        initial_hidden = tf.zeros([batch_size, self.hidden_dim], tf.float32)\n",
        "        ini_state_cell = tf.stack([initial_hidden, initial_hidden])\n",
        "\n",
        "        # Make scan_time have shape of (seq_length, batch_size, 1)\n",
        "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0], tf.shape(scan_time)[1], 1])\n",
        "        concat_input = tf.concat([scan_time, scan_input], 2)  # concat_input: (seq_length, batch_size, input_dim+1)\n",
        "\n",
        "        # Call TLSTM_Unit recursively for each time step (seq_length, 2, batch_size, hidden_dim)\n",
        "        packed_hidden_states = tf.scan(self.tlstm_unit, concat_input, initializer=ini_state_cell, name='states')\n",
        "\n",
        "        # Extract all the hidden states\n",
        "        all_states = packed_hidden_states[:, 0, :, :]  # (seq_length, batch_size, hidden_dim)\n",
        "        all_states = tf.transpose(all_states, perm=[1, 0, 2])  # (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        if self.return_sequence:\n",
        "            return all_states  # return all hidden states\n",
        "        else:\n",
        "            return all_states[:, -1, :]  # return last hidden state\n",
        "\n",
        "    def map_elapse_time(self, t):\n",
        "        \"\"\" Given the time intervals, compute the decay terms.\n",
        "        Parameters\n",
        "        ----------\n",
        "        t : tensor\n",
        "            input time intervals.\n",
        "        Returns\n",
        "        -------\n",
        "        tensor\n",
        "            The computed decay terms.\n",
        "        \"\"\"\n",
        "        c1 = tf.constant(1, dtype=tf.float32)\n",
        "        c2 = tf.constant(2.7183, dtype=tf.float32)\n",
        "\n",
        "        T = tf.math.divide(c1, tf.math.log(t + c2), name='Log_elapse_time')\n",
        "        Ones = tf.ones([1, self.hidden_dim], dtype=tf.float32)\n",
        "        T = tf.matmul(T, Ones)\n",
        "\n",
        "        return T\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = ({'hidden_dim': self.hidden_dim,\n",
        "                   'supports_masking': self.supports_masking,\n",
        "                   'kernel_initializer': self.kernel_initializer,\n",
        "                   'bias_initializer': self.bias_initializer,\n",
        "                   'return_sequence': self.return_sequence,\n",
        "                   'trainable': self.trainable,\n",
        "                   'name': self.name})\n",
        "\n",
        "        return config\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # %% run on dummy tensors\n",
        "    inp_dim = 2\n",
        "    hid_dim = 5\n",
        "\n",
        "    time_steps = 10\n",
        "    batchSize = 20\n",
        "\n",
        "    x_ = np.ones([batchSize, time_steps, inp_dim])\n",
        "    # leftpad sequences with zeros -> simulate variable length inputs in batch\n",
        "    # first 2 sequences have 2 leading zeros, next two 4 leading zeros, ...\n",
        "    x_[:2, :2, :] = 0\n",
        "    x_[2:4, :4, :] = 0\n",
        "    x_[4:6, :6, :] = 0\n",
        "    x_ = tf.convert_to_tensor(x_, dtype='float32')\n",
        "    dts = np.ones([batchSize, time_steps])\n",
        "    dts[:2, :2] = 0\n",
        "    dts[2:4, :4] = 0\n",
        "    dts[4:6, :6] = 0\n",
        "    dts = tf.convert_to_tensor(dts, dtype='float32')\n",
        "\n",
        "    # initialize TLSTM layer object\n",
        "    TLSTM_layer_ = TLSTM_layer(hid_dim, return_sequence=True)\n",
        "    hidden = TLSTM_layer_(x_, dts)\n",
        "    print(tf.shape(hidden))\n",
        "\n",
        "    TLSTM_layer_ = TLSTM_layer(hid_dim, return_sequence=False)\n",
        "    hidden = TLSTM_layer_(x_, dts)\n",
        "    print(tf.shape(hidden))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcM2CyzmKxlJ"
      },
      "source": [
        "!pip install kora -q\n",
        "from kora import drive\n",
        "drive.link_nbs()\n",
        "import import_ipynb\n",
        "import src\n",
        "'''!pip install scikit-learn==0.23.1\n",
        "!pip install imbalanced-learn==0.7.0'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmjDtqiYQJeY"
      },
      "source": [
        "#Dataset\n",
        "'''\n",
        "The full dataset can be downloaded following this Github link:\n",
        "\n",
        "Requena, B., Cassani, G., Tagliabue, J., Greco, C., & Lacasa, L. (2020).\n",
        "Shopper intentprediction from clickstream e-commerce data with minimal browsing information.\n",
        "https://github.com/coveooss/shopper-intent-prediction-nature-2020\n",
        "'''\n",
        "\n",
        "df = pd.read_csv('./release_10_23_2020.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG2yFyFo6i95"
      },
      "source": [
        " #DOWNLOADING DATASET AND SETTING UP NOTEBOOK\n",
        "\n",
        "!gdown --id 1GU_VafffNuwPvlnJNnMcKlDnSvEhFLEY\n",
        "\n",
        "df = pd.read_csv('./release_10_23_2020.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJHFZxt3AjPL"
      },
      "source": [
        "## Pre-processing function\n",
        "def preprocess(df):\n",
        "\n",
        "  # rename actions\n",
        "  df['product_action'] = df['product_action'].replace([np.nan,'detail', 'add', 'remove', 'click', 'purchase',],[1,2,3,4,5,7])\n",
        "\n",
        "  # make sequences\n",
        "  df = df.groupby('session_id_hash')[['product_action', 'server_timestamp_epoch_ms']].agg(list).reset_index()\n",
        "\n",
        "  #make labels\n",
        "  df['label'] = np.where(df.product_action.map(set([7]).issubset), 0, 1)\n",
        "  \n",
        "  #trim sequences after first purchase event\n",
        "  def trim(df):\n",
        "    sequence_action = []\n",
        "    timestamp = []\n",
        "    for item, time in zip(df.product_action, df.server_timestamp_epoch_ms):\n",
        "      if 7 in set(item):\n",
        "        for e in range(len(item)):\n",
        "          if item[e] == 7:\n",
        "            item1 = item[: e]\n",
        "            sequence_action.append(item1)\n",
        "            time1 = time[: e] \n",
        "            timestamp.append(time1)\n",
        "            break\n",
        "      else:\n",
        "        sequence_action.append(item)\n",
        "        timestamp.append(time)\n",
        "    df['sequence_action'] = sequence_action\n",
        "    df['timestamp'] = timestamp\n",
        "    return df\n",
        "  trim(df)\n",
        "\n",
        "  #rearrange column order and drop old product_action column\n",
        "  df = df[['session_id_hash', 'sequence_action', 'timestamp', 'label']]\n",
        "\n",
        "  #drop sequences shorter than 5 and longer than 155\n",
        "  index_to_drop = []\n",
        "  for id, row in df.iterrows():\n",
        "    if len(row[1]) < 5 or len(row[1]) > 155:\n",
        "      index_to_drop.append(id)\n",
        "  df = df.drop(df.index[index_to_drop])\n",
        "\n",
        "  #add exit event\n",
        "  def add_exit(df):\n",
        "    sequence_exit = []\n",
        "    for id, row in df.iterrows():\n",
        "      row[1] = [0] + row[1] + [6]\n",
        "      sequence_exit.append(row[1])\n",
        "    df['sequence_exit'] = sequence_exit\n",
        "    return df\n",
        "  add_exit(df)\n",
        "\n",
        "  # add durations\n",
        "  def delta(df):\n",
        "    delta = []\n",
        "    for t in df['timestamp']:\n",
        "      v = [0] + list(np.diff([t[0]] + t))\n",
        "      v = np.asarray(v)\n",
        "      delta.append(v)\n",
        "    df['delta'] = delta\n",
        "    return df\n",
        "  delta(df)\n",
        "  \n",
        "  #reset index\n",
        "  df = df[['session_id_hash', 'sequence_exit', 'timestamp', 'delta', 'label']]\n",
        "  df = df.reset_index()\n",
        "  df.drop('index', inplace=True, axis=1)\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_blMhKlRyRB"
      },
      "source": [
        "### Preprocess Dataframe\n",
        "df2 = preprocess(df)\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGZnkc861liR"
      },
      "source": [
        "#plot average time on site\n",
        "average_time_on_site = []\n",
        "for row in df2['timestamp']:\n",
        "  average_time_on_site.append(row[-1]-row[0])\n",
        "q25, q75 = np.percentile(average_time_on_site,[.25,.75])\n",
        "bin_width = 2*(q75 - q25)*len(average_time_on_site)**(-1/3)\n",
        "average_time_on_site = np.asarray(average_time_on_site)\n",
        "bins = round((average_time_on_site.max() - average_time_on_site.min())/bin_width)\n",
        "print(\"Freedman–Diaconis number of bins:\", bins)\n",
        "plt.hist(average_time_on_site, density=True, bins=100)  # density=False would make counts\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Time spent on the website');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB9f4qBE3z8l"
      },
      "source": [
        "average_time_on_site = np.asarray(average_time_on_site)\n",
        "average_time_on_site= average_time_on_site/1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb9d44YP5S6-"
      },
      "source": [
        "average_duration = []\n",
        "for row in df2['delta']:\n",
        "  row = np.asarray(row[2:])\n",
        "  mean = np.mean(row)\n",
        "  if mean < 500:\n",
        "    average_duration.append(mean)\n",
        "average_duration = np.asarray(average_duration)\n",
        "plt.hist(average_duration, density=False, bins=150, color = \"skyblue\", )  # density=False would make counts\n",
        "plt.ylabel('counts')\n",
        "plt.xlabel('average time spent on page');\n",
        "plt.title(\"Average time between consecutive actions\", size = 15, pad = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS4TEQpoAslV"
      },
      "source": [
        "average_d = pd.DataFrame()\n",
        "average_d['average duration'] = average_duration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qENpK_y37EHX"
      },
      "source": [
        "print(np.mean(average_duration))\n",
        "print(np.std(average_duration))\n",
        "print(max(average_duration))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXJbmw0oIcyU"
      },
      "source": [
        "#Bar plot\n",
        "view_c = 0\n",
        "detail_c= 0\n",
        "add_c= 0\n",
        "remove_c= 0\n",
        "click_c= 0\n",
        "view_nc= 0\n",
        "detail_nc= 0\n",
        "add_nc= 0\n",
        "remove_nc= 0\n",
        "click_nc= 0\n",
        "for id, row in df2.iterrows():\n",
        "  if row[4]==0:\n",
        "    for i in row[1]:\n",
        "      if i == 1:\n",
        "        view_c +=1\n",
        "      elif i == 2:\n",
        "        detail_c +=1\n",
        "      elif i == 3:\n",
        "        add_c +=1\n",
        "      elif i == 4:\n",
        "        remove_c +=1\n",
        "      elif i == 5:\n",
        "        click_c +=1\n",
        "  elif row[4]==1:\n",
        "    for i in row[1]:\n",
        "      if i == 1:\n",
        "        view_nc +=1\n",
        "      elif i == 2:\n",
        "        detail_nc +=1\n",
        "      elif i == 3:\n",
        "        add_nc +=1\n",
        "      elif i == 4:\n",
        "        remove_nc +=1\n",
        "      elif i == 5:\n",
        "        click_nc +=1\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conversion = (view_c,detail_c,add_c,remove_c,click_c)\n",
        "non_conversion = (view_nc,detail_nc,add_nc,remove_nc,click_nc)\n",
        "ind = ('view','detail','add','remove','click') # the x locations for the groups\n",
        "width = 0.35\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(ind, non_conversion, width, color='r')\n",
        "ax.bar(ind, conversion, width,bottom=non_conversion , color='b')\n",
        "ax.set_ylabel('Counts (in 100k)')\n",
        "ax.set_title('Action events number by class',size = 15, pad = 10)\n",
        "ax.set_xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))\n",
        "ax.set_yticks(np.arange(0, 3000000, 300000))\n",
        "ax.legend(labels=['Non-onversion', 'Conversion'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rpNsbsk0ZFf"
      },
      "source": [
        "## histogram\n",
        "length_sequences_C = []\n",
        "length_sequences_NC = []\n",
        "\n",
        "tot0 = df2[df2['label'] == 0].shape[0]\n",
        "tot1 = df2[df2['label'] == 1].shape[0]\n",
        "\n",
        "for item, y in zip(df2.sequence_exit, df2.label):\n",
        "  if y == 0:\n",
        "    length_sequences_C.append(len(item))\n",
        "  else:\n",
        "    length_sequences_NC.append(len(item))\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "plt.hist(length_sequences_NC, bins=150, alpha=0.4, label=\"Non- conversion sequences\", density=True, color = \"g\")\n",
        "plt.hist(length_sequences_C, bins=150, alpha=0.4, label=\"Conversion sequences\", density=True, color = \"orange\")\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.xlabel(\"Timesteps\", size=14)\n",
        "plt.ylabel(\"Probabilities\", size=14)\n",
        "plt.title(\"Histogram of Conversion and Non-conversion Trajectories \")\n",
        "plt.legend(loc='upper right')\n",
        "#plt.savefig(\"overlapping_histograms_with_matplotlib_Python.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM4vo-_Ol_4L"
      },
      "source": [
        "#plot class imbalance\n",
        "pd.value_counts(df_1['label']).plot.bar()\n",
        "plt.title('Label histogram')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Frequency')\n",
        "df_1['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vm_wnaL7U0Z"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "num_classes = 7\n",
        "maxlen = 157\n",
        "\n",
        "X1 = []\n",
        "for sequence in df2['sequence_exit']:\n",
        "  X1.append(to_categorical(sequence, num_classes = num_classes))\n",
        "lol = X1\n",
        "X1 = pad_sequences(X1, maxlen = maxlen, value = 0)\n",
        "X2 = pad_sequences(df2['delta'], maxlen -1, value = -1)\n",
        "X3 = df2['timestamp']\n",
        "\n",
        "new_X = []\n",
        "new_y = []\n",
        "for x in X1:\n",
        "  new_X.append(x[:-1])\n",
        "  new_y.append(x[1:])\n",
        "\n",
        "new_X = np.asarray(new_X)\n",
        "new_y = np.asarray(new_y)\n",
        "\n",
        "\n",
        "X = pd.DataFrame({'new_X': list(new_X), 'X2': list(X2), 'X3': list(X3), 'new_y': list(new_y)}, columns=['new_X', 'X2', 'X3', 'new_y'])\n",
        "\n",
        "y = df2['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbn_8kFrotRs"
      },
      "source": [
        "#test train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train1, X_val1, y_train1, y_val1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify = y)\n",
        "X_val2, X_test2, y_val2, y_test2 = train_test_split(X_val1, y_val1, test_size=0.5, random_state=42, stratify = y_val1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm3Ik66XpKFp"
      },
      "source": [
        "#label sets\n",
        "\n",
        "from numpy import where\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "y_train = pd.DataFrame()\n",
        "y_train['new_y'] = X_train1['new_y']\n",
        "\n",
        "y_train['y_label'] = y_train1\n",
        "\n",
        "y_val = pd.DataFrame()\n",
        "y_val['new_y'] = X_val2['new_y']\n",
        "\n",
        "y_val['y_label'] = y_val2\n",
        "\n",
        "y_test = pd.DataFrame()\n",
        "y_test['new_y'] = X_test2['new_y']\n",
        "\n",
        "y_test['y_label'] = y_test2\n",
        "\n",
        "# summarize the new class distribution\n",
        "counter = Counter(y_train)\n",
        "print(counter)\n",
        "\n",
        "print(len(X_train1))\n",
        "print(len(X_val2))\n",
        "print(len(X_test2))\n",
        "\n",
        "print(len(y_train1))\n",
        "print(len(y_val2))\n",
        "print(len(y_test2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiBKDFw8vYda"
      },
      "source": [
        "#reshape all data\n",
        "X_train_new = np.stack(X_train1['new_X']).reshape((len(X_train1['new_X']), (maxlen -1 ) , num_classes)).astype(np.float32)\n",
        "X_val_new = np.stack(X_val2['new_X']).reshape((len(X_val2['new_X']), (maxlen -1 ), num_classes)).astype(np.float32)\n",
        "X_test_new = np.stack(X_test2['new_X']).reshape((len(X_test2['new_X']), (maxlen -1), num_classes)).astype(np.float32)\n",
        "X_train_delta = np.stack(X_train1['X2']).reshape((len(X_train1['X2']), maxlen-1)).astype(np.float32)\n",
        "X_val_delta = np.stack(X_val2['X2']).reshape((len(X_val2['X2']),maxlen -1)).astype(np.float32)\n",
        "X_test_delta = np.stack(X_test2['X2']).reshape((len(X_test2['X2']),maxlen -1)).astype(np.float32)\n",
        "X_train_time = np.asarray(X_train1['X3'])\n",
        "X_val_time = np.asarray(X_val2['X3'])\n",
        "X_test_time = np.asarray(X_test2['X3'])\n",
        "\n",
        "y_train_label = y_train['y_label']\n",
        "y_val_label = y_val['y_label']\n",
        "y_test_label = y_test['y_label']\n",
        "\n",
        "y_train_new = np.stack(y_train['new_y']).reshape((len(y_train['new_y']), (maxlen -1 ), num_classes)).astype(np.float32)\n",
        "y_val_new = np.stack(y_val['new_y']).reshape((len(y_val['new_y']), (maxlen -1), num_classes)).astype(np.float32)\n",
        "y_test_new = np.stack(y_test['new_y']).reshape((len(y_test['new_y']), (maxlen -1 ), num_classes)).astype(np.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-qdN0fkPrwR"
      },
      "source": [
        "print(X_train_new.shape)\n",
        "print(X_val_new.shape)\n",
        "print(X_test_new.shape)\n",
        "print('---------------')\n",
        "print(X_train_delta.shape)\n",
        "print(X_val_delta.shape)\n",
        "print(X_test_delta.shape)\n",
        "print('---------------')\n",
        "print(y_train_new.shape)\n",
        "print(y_val_new.shape)\n",
        "print(y_test_new.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiWMfGvzX9D9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "d4622d7a-812f-4cf8-ba66-b2f48a6eaa06"
      },
      "source": [
        "#Discriminative specialised LSTM\n",
        "from keras.layers import LSTM, Dense, Masking, Input, BatchNormalization\n",
        "from keras import  Model\n",
        "\n",
        "main_input = Input(shape=(maxlen -1, num_classes), name='main_input')\n",
        "mask = Masking(mask_value = [0.,0.,0.,0.,0.,0.,0.], name = 'mask' )(main_input)\n",
        "# train a 2-layer LSTM with one shared layer\n",
        "l1 = LSTM(64, return_sequences=True)(mask)  # the shared layer\n",
        "b1 = BatchNormalization()(l1)\n",
        "\n",
        "l2_1 = LSTM(64, return_sequences=False)(b1)  # the layer specialized in activity prediction\n",
        "b2_1 = BatchNormalization()(l2_1)\n",
        "\n",
        "binary_output = Dense(1, activation='sigmoid', name='binary_output')(b2_1)\n",
        "\n",
        "model_SD = Model(inputs=[main_input], outputs=[binary_output])\n",
        "\n",
        "model_SD.compile(loss= 'binary_crossentropy',optimizer = 'adam', metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b8e3e42b9e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'main_input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mask'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train a 2-layer LSTM with one shared layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'maxlen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7GRbeksYRj3"
      },
      "source": [
        "#Fit model\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "history_SD = model_SD.fit(X_train_new, y_train_label,\n",
        "                         validation_data = (X_val_new, y_val_label),\n",
        "                         verbose=1,\n",
        "                         batch_size=batch_size,\n",
        "                         epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fKpqFhet3Ez"
      },
      "source": [
        "# model predict\n",
        "y_pred_binary_SD = model_SD.predict(X_test_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_tSkZj2uXtI"
      },
      "source": [
        "#save model\n",
        "model_SD.save('model_SD.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88Zg0Du9ujfs"
      },
      "source": [
        "#load model\n",
        "model_SD = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Thesis/model_SD.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZA_ErvQRRS7"
      },
      "source": [
        "#TIME generative specialised TIME LSTM\n",
        "from keras.layers import LSTM, Dense, Masking, Input, BatchNormalization\n",
        "from keras import  Model\n",
        "\n",
        "main_input = Input(shape=(maxlen -1, num_classes), name='main_input')\n",
        "mask1 = Masking(mask_value = 0, name = 'mask1' )(main_input)\n",
        "# delta_ts: (batch_size, 1)\n",
        "delta_ts = Input(shape=(maxlen -1, ), name='delta_ts')\n",
        "mask2 = Masking(mask_value = -1, name = 'mask2' )(delta_ts)\n",
        "# train a 2-layer LSTM with one shared layer\n",
        "l1 = TLSTM_layer(64, return_sequence=True)(mask1,mask2)  # the shared layer\n",
        "b1 = BatchNormalization()(l1)\n",
        "\n",
        "l2_1 = TLSTM_layer(64, return_sequence=True)(b1,mask2)  # the layer specialized in activity prediction\n",
        "b2_1 = BatchNormalization()(l2_1)\n",
        "\n",
        "act_output = Dense(num_classes, activation='softmax', name='act_output')(b2_1)\n",
        "\n",
        "time_model_SG = Model(inputs=[main_input, delta_ts], outputs=[act_output])\n",
        "\n",
        "time_model_SG.compile(loss= 'categorical_crossentropy',optimizer = 'adam', metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW-9MdcLSO88"
      },
      "source": [
        "#Fit model\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "history_time_SG = time_model_SG.fit([X_train_new, X_train_delta], y_train_new,\n",
        "                         validation_data = ([X_val_new, X_val_delta], y_val_new),\n",
        "                         verbose=1,\n",
        "                         batch_size=batch_size,\n",
        "                         epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XylroSssPfu"
      },
      "source": [
        "y_pred_act_TIME_SG= time_model_SG.predict([X_test_new, X_test_delta])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byh_QyjOrmfG",
        "outputId": "b3d1d869-135b-469d-a7e8-46b608603b52"
      },
      "source": [
        "time_model_SG.save('time_model_SG.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: time_model_SG.tf/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww1BHJA1uw-c"
      },
      "source": [
        "from src import TLSTM_layer\n",
        "time_model_SG = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Thesis/time_model_SG.h5', custom_objects={'TLSTM_layer':TLSTM_layer})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkJgBEuALzky"
      },
      "source": [
        "#TIME discriminative specialised TIME LSTM\n",
        "from keras.layers import LSTM, Dense, Masking, Input, BatchNormalization, Lambda\n",
        "from keras import  Model\n",
        "from src import TLSTM_layer\n",
        "main_input = Input(shape=(maxlen -1, num_classes), name='main_input')\n",
        "mask1 = Masking(mask_value = 0, name = 'mask1' )(main_input)\n",
        "# delta_ts: (batch_size, 1)\n",
        "delta_ts = Input(shape=(maxlen -1, ), name='delta_ts')\n",
        "mask2 = Masking(mask_value = -1, name = 'mask2' )(delta_ts)\n",
        "# train a 2-layer LSTM with one shared layer\n",
        "l1 = TLSTM_layer(64, return_sequence=True)(mask1,mask2)  # the shared layer\n",
        "b1 = BatchNormalization()(l1)\n",
        "\n",
        "l2_2a = TLSTM_layer(64, return_sequence=True)(b1, mask2)\n",
        "l2_2 = Lambda(lambda x: x[:, -1, :])(l2_2a)\n",
        "b2_1 = BatchNormalization()(l2_2)\n",
        "\n",
        "binary_output = Dense(1, activation='sigmoid', name='act_output')(b2_1)\n",
        "\n",
        "time_model_SD = Model(inputs=[main_input, delta_ts], outputs=[binary_output])\n",
        "\n",
        "time_model_SD.compile(loss= 'binary_crossentropy',optimizer = 'adam', metrics = ['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGiPPqMSMMFi"
      },
      "source": [
        "#Fit model\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "history_time_SD = time_model_SD.fit([X_train_new, X_train_delta], y_train_label,\n",
        "                         validation_data = ([X_val_new, X_val_delta], y_val_label),\n",
        "                         verbose=1,\n",
        "                         batch_size=batch_size,\n",
        "                         epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz-xx0w2FnB3"
      },
      "source": [
        "# model predict\n",
        "y_pred_binary_TIME_SD = time_model_SD.predict([X_test_new, X_test_delt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpm1N5tcmuoR"
      },
      "source": [
        "time_model_SD.save('time_model_SD.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "JtNJ_IlXFNOc",
        "outputId": "d62d5302-106c-49d5-9cc7-e113c633f2df"
      },
      "source": [
        "time_model_SD = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Thesis/time_model_SD.h5',custom_objects={'TLSTM_layer':TLSTM_layer})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-765533b3cf8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_model_SD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Thesis/time_model_SD.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'TLSTM_layer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTLSTM_layer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW51k9h5X-Qm"
      },
      "source": [
        "#Generative specialised LSTM\n",
        "from keras.layers import LSTM, Dense, Masking, Input, BatchNormalization\n",
        "from keras import  Model\n",
        "\n",
        "main_input = Input(shape=(maxlen -1, num_classes), name='main_input')\n",
        "mask = Masking(mask_value = [0.,0.,0.,0.,0.,0.,0.], name = 'mask' )(main_input)\n",
        "# train a 2-layer LSTM with one shared layer\n",
        "l1 = LSTM(64, return_sequences=True)(mask)  # the shared layer\n",
        "b1 = BatchNormalization()(l1)\n",
        "\n",
        "l2_1 = LSTM(64, return_sequences=True)(b1)  # the layer specialized in activity prediction\n",
        "b2_1 = BatchNormalization()(l2_1)\n",
        "\n",
        "act_output = Dense(num_classes, activation='softmax', name='act_output')(b2_1)\n",
        "\n",
        "model_SG = Model(inputs=[main_input], outputs=[act_output])\n",
        "\n",
        "model_SG.compile(loss= 'categorical_crossentropy',optimizer = 'adam', metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqXe-AsbHZii"
      },
      "source": [
        "#Fit model\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "history = model_SG.fit(X_train_new, y_train_new,\n",
        "                         validation_data = (X_val_new, y_val_new),\n",
        "                         verbose=1,\n",
        "                         batch_size=batch_size,\n",
        "                         epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3xQ6ZWcHx0K"
      },
      "source": [
        "# model predict\n",
        "y_pred_act_SG = model_SG.predict(X_test_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKtJ3AosvYcp"
      },
      "source": [
        "#save model\n",
        "model_SG.save('model_SG.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBLD23p3uirv"
      },
      "source": [
        "model_SG = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Thesis/model_SG.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftpb-d5U5bGz"
      },
      "source": [
        "## Gen and discriminative LSTM\n",
        "from keras.layers import LSTM, Dense, Masking, Input, BatchNormalization\n",
        "from keras import  Model\n",
        "\n",
        "main_input = Input(shape=(maxlen -1, num_classes), name='main_input')\n",
        "mask = Masking(mask_value = [0.,0.,0.,0.,0.,0.,0.], name = 'mask' )(main_input)\n",
        "# train a 2-layer LSTM with one shared layer\n",
        "l1 = LSTM(64, return_sequences=True)(mask)  # the shared layer\n",
        "b1 = BatchNormalization()(l1)\n",
        "\n",
        "l2_1 = LSTM(64, return_sequences=True)(b1)  # the layer specialized in activity prediction\n",
        "b2_1 = BatchNormalization()(l2_1)\n",
        "\n",
        "l2_2 = LSTM(64)(b1)  # the layer specialized in classifcation\n",
        "b2_2 = BatchNormalization()(l2_2)\n",
        "\n",
        "act_output = Dense(num_classes, activation='softmax', name='act_output')(b2_1)\n",
        "binary_output = Dense(1, activation='sigmoid',name='binary_output')(b2_2)\n",
        "\n",
        "model = Model(inputs=[main_input], outputs=[act_output, binary_output])\n",
        "model.compile(loss={'act_output': 'categorical_crossentropy', 'binary_output': 'binary_crossentropy'},\n",
        "                      optimizer = 'adam', metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnE-6_8iLhVC"
      },
      "source": [
        "#Fit model\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "history = model.fit(X_train_new,{'act_output': y_train_new, 'binary_output': y_train_label},\n",
        "                         validation_data = (X_val_new,{'act_output': y_val_new, 'binary_output': y_val_label}),\n",
        "                         verbose=1,\n",
        "                         batch_size=batch_size,\n",
        "                         epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k4-mFSRhB1o"
      },
      "source": [
        "#Plot loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LSTM loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20OCrE_wfXNz"
      },
      "source": [
        "# model predict\n",
        "y_pred_act1, y_pred_binary1 = model.predict(X_test_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg7XfCcAWFyL"
      },
      "source": [
        "#save model\n",
        "model.save('vanilla.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKFNPZbFWEou"
      },
      "source": [
        "#load model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/vanilla.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hWIAtNEWV_7"
      },
      "source": [
        "#discriminative report\n",
        "import sklearn\n",
        "print(sklearn.metrics.classification_report(y_test_label, t_y_pred_binary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWPdu5OZWcvH"
      },
      "source": [
        "# autoregressive report\n",
        "print(sklearn.metrics.classification_report(y_test_new, y_pred_act))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmsnqVoDfxXa"
      },
      "source": [
        "#Plot probability of exit for one trajectory:\n",
        "def get_x_y(number):\n",
        "  one = []\n",
        "  for item in y_pred_act[number]:\n",
        "    one.append(item[-1])\n",
        "\n",
        "  item = [np.argmax(y, axis=None, out=None) for y in X_test_new[number]]\n",
        "  item = np.ma.masked_equal(item, 0)\n",
        "  item = item.compressed()\n",
        "  lol = []\n",
        "  for e in item:\n",
        "    lol.append(e)\n",
        "  x = []\n",
        "  for i in range(len(lol)):\n",
        "    x.append(i)\n",
        "\n",
        "  y = one[-len(lol):]\n",
        "\n",
        "  return x, y\n",
        "x, y = get_x_y(9877)\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u24pqfUFsv8"
      },
      "source": [
        "## Time aware Gen and discriminative LSTM \n",
        "from keras.layers import LSTM, Dense, Masking, Input, BatchNormalization, Lambda\n",
        "from keras import  Model\n",
        "\n",
        "main_input = Input(shape = (maxlen - 1, num_classes), name = 'main_input')\n",
        "mask1 = Masking(mask_value = 0, name = 'mask1' )(main_input)\n",
        "# delta_ts: (batch_size, 1)\n",
        "delta_ts = Input(shape=(maxlen -1, ), name='delta_ts')\n",
        "mask2 = Masking(mask_value = -1, name = 'mask2' )(delta_ts)\n",
        "\n",
        "l1 = TLSTM_layer(64, return_sequence = True)(mask1, mask2)\n",
        "\n",
        "# batchnorm shared TLSTM layer output\n",
        "b1 = keras.layers.BatchNormalization()(l1)\n",
        "\n",
        "# the layer specialized in time prediction\n",
        "l2_1 = TLSTM_layer(64, return_sequence = True)(b1, mask2)\n",
        "\n",
        "# batchnorm activity prediction specialized LSTM\n",
        "b2_1 = keras.layers.BatchNormalization()(l2_1)\n",
        "\n",
        "# the layer specialized in binary classification\n",
        "l2_2a = TLSTM_layer(64)(b1, mask2)\n",
        "l2_2 = Lambda(lambda x: x[:, -1, :])(l2_2a)\n",
        "\n",
        "# batchnorm time prediction specialized TLSTM output\n",
        "b2_2 = keras.layers.BatchNormalization()(l2_2)\n",
        "\n",
        "#d2_1 = keras.layers.Dropout(rate=hparams[HP_DROPOUT])(b2_1)\n",
        "#d2_2 = keras.layers.Dropout(rate=hparams[HP_DROPOUT])(b2_2)\n",
        "\n",
        "act_output = Dense(num_classes, activation = 'softmax', name='act_output')(b2_1)\n",
        "binary_output = Dense(1, activation = 'sigmoid', name='binary_output')(b2_2)\n",
        "\n",
        "# define model\n",
        "time_model = Model(inputs=[main_input, delta_ts], outputs=[act_output, binary_output])\n",
        "\n",
        "# compile model\n",
        "time_model.compile(loss={'act_output': 'categorical_crossentropy', 'binary_output': 'binary_crossentropy'},\n",
        "                      optimizer = 'adam', metrics = ['acc'])\n",
        "time_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NouKER8zJa7U"
      },
      "source": [
        "from IPython.display       import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(time_model, show_shapes = True, show_layer_names = True, dpi=65).create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvAVb38JxGl"
      },
      "source": [
        "#fit time model\n",
        "batch_size = 64,\n",
        "epochs = 5\n",
        "history_time = time_model.fit([X_train_new, X_train_delta],\n",
        "                          [y_train_new, y_train_label],\n",
        "                         validation_data = ([X_val_new, X_val_delta],\n",
        "                                             [y_val_new, y_val_label]),\n",
        "                         verbose = 1,\n",
        "                         batch_size = 64,\n",
        "                         epochs = epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F-1RjCIWZXs"
      },
      "source": [
        "#Plot loss\n",
        "plt.plot(history_time.history['loss'])\n",
        "plt.plot(history_time.history['val_loss'])\n",
        "plt.title('LSTM loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvNa8usuU2Xt"
      },
      "source": [
        "y_pred_act, y_pred_binary = time_model.predict([X_test_new, X_test_delta])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHx9lP3BRkNz"
      },
      "source": [
        "# save model\n",
        "time_model.save('time_model_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiW8Khh9VMzB"
      },
      "source": [
        "#load model\n",
        "time_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Thesis/time_model_new.h5', custom_objects= {'TLSTM_layer':TLSTM_layer})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yCRrrZBVhC4"
      },
      "source": [
        "#discriminative report\n",
        "sklearn.metrics.classification_report(y_test_label, y_pred_binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXTlbBioVyCF"
      },
      "source": [
        "# autoregressive report\n",
        "sklearn.metrics.classification_report(y_test_new, y_pred_act)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dyFDYJVWxn7"
      },
      "source": [
        "#Plot probability of exit for one trajectory:\n",
        "def get_x_y(number):\n",
        "  one = []\n",
        "  for item in y_pred_act_TIME_SG[number]:\n",
        "    one.append(item[-1])\n",
        "\n",
        "  item = [np.argmax(y, axis=None, out=None) for y in X_test_new[number]]\n",
        "  item = np.ma.masked_equal(item, 0)\n",
        "  item = item.compressed()\n",
        "  lol = []\n",
        "  for e in item:\n",
        "    lol.append(e)\n",
        "  x = []\n",
        "  for i in range(len(lol)):\n",
        "    x.append(i)\n",
        "\n",
        "  y = one[-len(lol):]\n",
        "\n",
        "  return x, y\n",
        "x, y = get_x_y(14002)\n",
        "plt.plot(x, y, label = 'exit risk' )\n",
        "plt.title('Single task Time LSTM')\n",
        "plt.xlabel(\"timesteps\", size=14)\n",
        "plt.ylabel(\"risk\", size=14)\n",
        "plt.legend(loc=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RDmRdUQcgKZ"
      },
      "source": [
        "#Recall, F1-score, AUC ROC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print('-----------')\n",
        "print('AUC ROC')\n",
        "print('Specialized LSTM')\n",
        "print(roc_auc_score(y_test_label, y_pred_binary_SD))\n",
        "print('------------')\n",
        "print('Multi LSTM')\n",
        "print(roc_auc_score(y_test_label, y_pred_binary1))\n",
        "print('------------')\n",
        "print('Multi Time LSTM')\n",
        "print(roc_auc_score(y_test_label, y_pred_binary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uURj9gdGnEmq"
      },
      "source": [
        "#Perplexity\n",
        "from keras import backend as K\n",
        "def ppl_e(y_true, y_pred):\n",
        "    return K.exp(K.mean(K.categorical_crossentropy(y_true, y_pred)))\n",
        "print('-------------')\n",
        "print('Time LSTM')\n",
        "print(ppl_e(y_test_new,y_pred_act))\n",
        "print('-------------')\n",
        "print('Multi LSTM')\n",
        "print(ppl_e(y_test_new,y_pred_act1))\n",
        "print('--------------')\n",
        "print('Specialized LSTM')\n",
        "print(ppl_e(y_test_new,y_pred_act_SG))\n",
        "print('--------------')\n",
        "print('Specialized TIME LSTM')\n",
        "print(ppl_e(y_test_new,y_pred_act_TIME_SG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOcq8qEJtfWY"
      },
      "source": [
        "#ROC CURVE MULTIPLR\n",
        "from sklearn import metrics\n",
        "plt.figure(0).clf()\n",
        "\n",
        "fpr, tpr, thresh = metrics.roc_curve(y_test_label, y_pred_binary_SD)\n",
        "auc = metrics.roc_auc_score(y_test_label, y_pred_binary_SD)\n",
        "plt.plot(fpr,tpr,label=\"LSTM_S, auc=\"+str(auc))\n",
        "\n",
        "fpr, tpr, thresh = metrics.roc_curve(y_test_label,y_pred_binary1)\n",
        "auc = metrics.roc_auc_score(y_test_label, y_pred_binary1)\n",
        "plt.plot(fpr,tpr,label=\"Multi-task LSTM, auc=\"+str(auc))\n",
        "\n",
        "fpr, tpr, thresh = metrics.roc_curve(y_test_label, y_pred_binary)\n",
        "auc = metrics.roc_auc_score(y_test_label, y_pred_binary)\n",
        "plt.plot(fpr,tpr,label=\"Multi_task T-LSTM, auc=\"+str(auc))\n",
        "plt.xlabel(\"FPR\", size=14)\n",
        "plt.ylabel(\"TPR\", size=14)\n",
        "plt.title(\"ROC curves\")\n",
        "plt.plot([0,1], [0,1], linestyle='--', label='No Skill, auc =0.5')\n",
        "plt.legend(loc=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTNSoZKBrbVN"
      },
      "source": [
        "#POst-Processsing\n",
        "#random choice\n",
        "\n",
        "def make_y_sequences(y_pred_act):\n",
        "  random_pred = []\n",
        "  real_pred = []\n",
        "  for i in range(len(y_pred_act)):\n",
        "    item = [np.argmax(y, axis=None, out=None) for y in y_test_new[i]]\n",
        "    item = np.ma.masked_equal(item, 0)\n",
        "    item = item.compressed()\n",
        "    real_pred.append(item)\n",
        "    l = len(item)\n",
        "    ns = []\n",
        "    for r in y_pred_act[i][-l:]:\n",
        "      wow =np.random.choice(r, p=r)\n",
        "      for i in range(len(r)):\n",
        "        if r[i] == wow:\n",
        "          ns.append(i)\n",
        "    random_pred.append(ns)\n",
        "  return random_pred, real_pred\n",
        "random_pred, real_pred = make_y_sequences(y_pred_act)\n",
        "random_pred1, real_pred = make_y_sequences(y_pred_act1)\n",
        "random_pred_SG, real_pred = make_y_sequences(y_pred_act_SG)\n",
        "random_pred_TIME_SG, real_pred = make_y_sequences(y_pred_act_TIME_SG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5kUTNylpSA2"
      },
      "source": [
        "#DL distance from https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
        "import numpy as np\n",
        "\n",
        "def levenshtein(seq1, seq2):\n",
        "    size_x = len(seq1) + 1\n",
        "    size_y = len(seq2) + 1\n",
        "    matrix = np.zeros ((size_x, size_y))\n",
        "    for x in range(size_x):\n",
        "        matrix [x, 0] = x\n",
        "    for y in range(size_y):\n",
        "        matrix [0, y] = y\n",
        "\n",
        "    for x in range(1, size_x):\n",
        "        for y in range(1, size_y):\n",
        "            if seq1[x-1] == seq2[y-1]:\n",
        "                matrix [x,y] = min(\n",
        "                    matrix[x-1, y] + 1,\n",
        "                    matrix[x-1, y-1],\n",
        "                    matrix[x, y-1] + 1\n",
        "                )\n",
        "            else:\n",
        "                matrix [x,y] = min(\n",
        "                    matrix[x-1,y] + 1,\n",
        "                    matrix[x-1,y-1] + 1,\n",
        "                    matrix[x,y-1] + 1\n",
        "                )\n",
        "    return (matrix[size_x - 1, size_y - 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJPsnemno_sE",
        "outputId": "eb9b2839-8648-433f-f781-f970414f5b4e"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, MaxPooling2D\n",
        "from keras.layers.convolutional import Conv2D \n",
        "from keras.layers.recurrent import SimpleRNN, LSTM, GRU \n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "\n",
        "from distutils.version import LooseVersion as LV\n",
        "from keras import __version__\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "from keras.datasets import mnist, fashion_mnist, imdb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
        "assert(LV(__version__) >= LV(\"2.0.0\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Keras version: 2.4.3 backend: tensorflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6dp_GGhn0jP",
        "outputId": "74c227d5-f578-49fd-9af2-e0f5bd89e4b0"
      },
      "source": [
        "if K.backend() == \"tensorflow\":\n",
        "    import tensorflow as tf\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name == '':\n",
        "        device_name = \"None\"\n",
        "    print('Using TensorFlow version:', tf.__version__, ', GPU:', device_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow version: 2.4.1 , GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0HZwlgiGIUQ"
      },
      "source": [
        "#distance similarity array\n",
        "distances = []\n",
        "distances1 = []\n",
        "distances_SG = []\n",
        "distances_TIME_SG = []\n",
        "for ps, rs in zip(random_pred, real_pred):\n",
        "    distances.append(levenshtein(ps, rs))\n",
        "for ps, rs in zip(random_pred1, real_pred):\n",
        "    distances1.append(levenshtein(ps, rs))\n",
        "for ps, rs in zip(random_pred_SG, real_pred):\n",
        "    distances_SG.append(levenshtein(ps, rs))\n",
        "for ps, rs in zip(random_pred_TIME_SG, real_pred):\n",
        "    distances_TIME_SG.append(levenshtein(ps, rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyx0JkCErMY4"
      },
      "source": [
        "# DL mean\n",
        "print('DL mean')\n",
        "print('---------')\n",
        "print('Multi-task TIME lstm')\n",
        "print(np.mean(distances))\n",
        "print('---------')\n",
        "print('Multi-task lstm')\n",
        "print(np.mean(distances1))\n",
        "print('---------')\n",
        "print('Single-task lstm')\n",
        "print(np.mean(distances_SG))\n",
        "print('---------')\n",
        "print('Single-task TIME lstm')\n",
        "print(np.mean(distances_TIME_SG))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhwNO1nFDa1j"
      },
      "source": [
        "#---------------END OF CODE---------------------#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}